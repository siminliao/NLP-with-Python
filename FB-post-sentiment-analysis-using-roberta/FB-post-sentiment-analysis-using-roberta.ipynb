{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb","timestamp":1680921556250}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e491b8de82f24c08bfcdb83b5af44118":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61ae3b7c203f457081e3a9e0a113a528","IPY_MODEL_0cc1eb9a27754a4287ad9afb58352563","IPY_MODEL_eba28e9f29cf4d6b894efbe969c22303"],"layout":"IPY_MODEL_a86812007e3c4be1af33284a55a4c86c"}},"61ae3b7c203f457081e3a9e0a113a528":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e847894ccf5b4b3180a724825af856f9","placeholder":"​","style":"IPY_MODEL_f6b6f3b8908649f2aa4f7891f9c645a3","value":"Downloading (…)olve/main/vocab.json: 100%"}},"0cc1eb9a27754a4287ad9afb58352563":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec69520b9572473995124fe246be2494","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1c79d04d3df40f983c982bd0c960c61","value":898823}},"eba28e9f29cf4d6b894efbe969c22303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08a6b50b08c14091be71857510382fd4","placeholder":"​","style":"IPY_MODEL_1afa23a2b9404bf994aa4923cf367950","value":" 899k/899k [00:00&lt;00:00, 1.07MB/s]"}},"a86812007e3c4be1af33284a55a4c86c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e847894ccf5b4b3180a724825af856f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6b6f3b8908649f2aa4f7891f9c645a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec69520b9572473995124fe246be2494":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1c79d04d3df40f983c982bd0c960c61":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08a6b50b08c14091be71857510382fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1afa23a2b9404bf994aa4923cf367950":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da5e305b5638476f9566d6e498097a0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3402efffb9704739a642ff05b08ed461","IPY_MODEL_23c9582f46df4abe9a633e0951c3eb23","IPY_MODEL_03d492af1a0849598f7d15c87fcd38a0"],"layout":"IPY_MODEL_4b4d24538f7d4d2192cb1094ae465f66"}},"3402efffb9704739a642ff05b08ed461":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ec1a29dc174e29b997e5675dd414ea","placeholder":"​","style":"IPY_MODEL_35a67d2c1a5d4f54a4006b4a9c2f27ad","value":"Downloading (…)olve/main/merges.txt: 100%"}},"23c9582f46df4abe9a633e0951c3eb23":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67154505e19e4c6a9a5fc0143e3c2eab","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_893c8741a91a4097989a2488034f3e39","value":456318}},"03d492af1a0849598f7d15c87fcd38a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1484d75fcf804155b7b98271a45dc79a","placeholder":"​","style":"IPY_MODEL_bbf31ba71eaa429bb72476a5cabe1bb2","value":" 456k/456k [00:00&lt;00:00, 728kB/s]"}},"4b4d24538f7d4d2192cb1094ae465f66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ec1a29dc174e29b997e5675dd414ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35a67d2c1a5d4f54a4006b4a9c2f27ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67154505e19e4c6a9a5fc0143e3c2eab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"893c8741a91a4097989a2488034f3e39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1484d75fcf804155b7b98271a45dc79a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf31ba71eaa429bb72476a5cabe1bb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86554572c2284797ab595dad4d1beb2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f2a2860d4f64724b1592e170f25bdc7","IPY_MODEL_16d02ec254674e2294e1349601f3f678","IPY_MODEL_01547edcd8074edea595f8cc278a7b7f"],"layout":"IPY_MODEL_762d19d010da4b8b85d2a36e6a883e78"}},"3f2a2860d4f64724b1592e170f25bdc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c15093fa7fd442ea2a923ad560b0ada","placeholder":"​","style":"IPY_MODEL_c455fd38879049c0bc301a4f76aae3b9","value":"Downloading (…)lve/main/config.json: 100%"}},"16d02ec254674e2294e1349601f3f678":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62a2f23119cf4e1e933a684294d00903","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bcf9f60e3524477a55a2049b4f7b620","value":481}},"01547edcd8074edea595f8cc278a7b7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4640fa5598e48da8004696d848bfe05","placeholder":"​","style":"IPY_MODEL_5b25d7d1538049e9b1d3b913f6f924e9","value":" 481/481 [00:00&lt;00:00, 29.7kB/s]"}},"762d19d010da4b8b85d2a36e6a883e78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c15093fa7fd442ea2a923ad560b0ada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c455fd38879049c0bc301a4f76aae3b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62a2f23119cf4e1e933a684294d00903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bcf9f60e3524477a55a2049b4f7b620":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f4640fa5598e48da8004696d848bfe05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b25d7d1538049e9b1d3b913f6f924e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"121a2cb0dc674ec7a1cecde46c12b8bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_796bd18f5b7748ffa1bcca749b0b1d26","IPY_MODEL_861498f14a524367bba0853816c87b52","IPY_MODEL_eb57666612e64c2ab1e68d3db631571e"],"layout":"IPY_MODEL_174073ef46ef4a558fb00dbcb01a8c18"}},"796bd18f5b7748ffa1bcca749b0b1d26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c43d9fa9cef49a78b6e6b270acaec86","placeholder":"​","style":"IPY_MODEL_107a6093636645e8af4837fec96a7909","value":"Downloading pytorch_model.bin: 100%"}},"861498f14a524367bba0853816c87b52":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5c5bc917b82423c8c925682e15313e4","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b362a762ae504bfda906f651f59ffa0a","value":501200538}},"eb57666612e64c2ab1e68d3db631571e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_908ff024d1e24521b7b9d832cbeb38d9","placeholder":"​","style":"IPY_MODEL_d5ef88169455412997145c5d8cdd082d","value":" 501M/501M [00:01&lt;00:00, 304MB/s]"}},"174073ef46ef4a558fb00dbcb01a8c18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c43d9fa9cef49a78b6e6b270acaec86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"107a6093636645e8af4837fec96a7909":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5c5bc917b82423c8c925682e15313e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b362a762ae504bfda906f651f59ffa0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"908ff024d1e24521b7b9d832cbeb38d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ef88169455412997145c5d8cdd082d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WTdfPjhFqExX"},"source":["### Readme\n","\n","1. [Importing Python Libraries and preparing the environment](#section01)\n","2. [Importing and Pre-Processing the domain data](#section02)\n","3. [Preparing the Dataset and Dataloader](#section03)\n","4. [Creating the Neural Network for Fine Tuning](#section04)\n","5. [Fine Tuning the Model](#section05)\n","6. [Validating the Model Performance](#section06)\n","7. [Saving the model and artifacts for Inference in Future](#section07)\n","8. [Make Predictions using the model](#section08)\n","\n","- Language Model Used:\n","\t- The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google’s BERT model released in 2018.\n","\t- [Blog-Post](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n","\t- [Research Paper](https://arxiv.org/pdf/1907.11692)\n","\t- [Documentation for python](https://huggingface.co/transformers/model_doc/roberta.html)\n","\n","\n","- Hardware Requirements:\n","\t- Python 3.6 and above\n","\t- Pytorch, Transformers and All the stock Python ML Libraries\n","\t- GPU enabled setup "]},{"cell_type":"markdown","metadata":{"id":"97CEi-bdqExb"},"source":["<a id='section01'></a>\n","### Importing Python Libraries and preparing the environment"]},{"cell_type":"code","metadata":{"id":"a-GlywkSFegL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680988463546,"user_tz":300,"elapsed":12693,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}},"outputId":"c30d8e68-cfc3-4328-d98b-33b499117202"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"]}]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"e7b5f5ab6f8f300c8900321a91b9340376c986f2","id":"979OUro5Eac3"},"source":["# Importing the libraries needed\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","import seaborn as sns\n","import transformers\n","import json\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaModel, RobertaTokenizer\n","import logging\n","logging.basicConfig(level=logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sb1Q5N6LGK7z"},"source":["# Setting up the device for GPU usage\n","\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3Q9NDdmqEyo"},"source":["<a id='section03'></a>\n","### Preparing the Dataset and Dataloader\n"]},{"cell_type":"code","source":["# Data is ~8k Facebook posts labeled into 3 exclusive classes\n","data = pd.read_csv('FB_posts_labeled.txt', delimiter='\\t')\n","# Create label encoder for 3 classes\n","data['label'] = 0\n","data.loc[data['Appreciation'] == 1, 'label'] = 1\n","data.loc[data['Complaint'] == 1, 'label'] = 2\n","data.loc[data['Feedback'] == 1, 'label'] = 3\n","data = data[['message','label']]\n","data = data.rename(columns={'message':'Phrase', 'label':'Sentiment'})\n","new_df = data[['Phrase', 'Sentiment']]"],"metadata":{"id":"hEZHdKDYwF4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvXxpfNCGER2","outputId":"12f33c38-c546-4520-a81b-9746cd588396","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["e491b8de82f24c08bfcdb83b5af44118","61ae3b7c203f457081e3a9e0a113a528","0cc1eb9a27754a4287ad9afb58352563","eba28e9f29cf4d6b894efbe969c22303","a86812007e3c4be1af33284a55a4c86c","e847894ccf5b4b3180a724825af856f9","f6b6f3b8908649f2aa4f7891f9c645a3","ec69520b9572473995124fe246be2494","a1c79d04d3df40f983c982bd0c960c61","08a6b50b08c14091be71857510382fd4","1afa23a2b9404bf994aa4923cf367950","da5e305b5638476f9566d6e498097a0c","3402efffb9704739a642ff05b08ed461","23c9582f46df4abe9a633e0951c3eb23","03d492af1a0849598f7d15c87fcd38a0","4b4d24538f7d4d2192cb1094ae465f66","a0ec1a29dc174e29b997e5675dd414ea","35a67d2c1a5d4f54a4006b4a9c2f27ad","67154505e19e4c6a9a5fc0143e3c2eab","893c8741a91a4097989a2488034f3e39","1484d75fcf804155b7b98271a45dc79a","bbf31ba71eaa429bb72476a5cabe1bb2","86554572c2284797ab595dad4d1beb2c","3f2a2860d4f64724b1592e170f25bdc7","16d02ec254674e2294e1349601f3f678","01547edcd8074edea595f8cc278a7b7f","762d19d010da4b8b85d2a36e6a883e78","2c15093fa7fd442ea2a923ad560b0ada","c455fd38879049c0bc301a4f76aae3b9","62a2f23119cf4e1e933a684294d00903","4bcf9f60e3524477a55a2049b4f7b620","f4640fa5598e48da8004696d848bfe05","5b25d7d1538049e9b1d3b913f6f924e9"]},"executionInfo":{"status":"ok","timestamp":1680988467822,"user_tz":300,"elapsed":3161,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["# Defining some key variables that will be used later on in the training\n","MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 4\n","# EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e491b8de82f24c08bfcdb83b5af44118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5e305b5638476f9566d6e498097a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86554572c2284797ab595dad4d1beb2c"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"3vWRDemOGxJD"},"source":["class SentimentData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.Phrase\n","        self.targets = self.data.Sentiment\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Gpe9D1QHoCd","outputId":"e1ab141d-4670-41f4-c58f-d6f5a2425eed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680988467823,"user_tz":300,"elapsed":8,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["train_size = 0.8\n","train_data=new_df.sample(frac=train_size,random_state=200)\n","test_data=new_df.drop(train_data.index).reset_index(drop=True)\n","train_data = train_data.reset_index(drop=True)\n","\n","\n","print(\"FULL Dataset: {}\".format(new_df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_data.shape))\n","print(\"TEST Dataset: {}\".format(test_data.shape))\n","\n","training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n","testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FULL Dataset: (7961, 2)\n","TRAIN Dataset: (6369, 2)\n","TEST Dataset: (1592, 2)\n"]}]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"9fc198d13d7f33dc70588c3f22bc7b7c4f4ebb45","id":"c1tInLk2Eadt"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZk0A9K8qE0C"},"source":["<a id='section04'></a>\n","### Creating the Neural Network for Fine Tuning\n","\n","#### Neural Network\n"," - Create a neural network with the `RobertaClass`\n"," - This network will have the Roberta Language model followed by a `dropout` and finally a `Linear` layer to obtain the final outputs. \n"," \n","#### Loss Function and Optimizer\n"," - The `Loss Function` is used the calculate the difference in the output created by the model and the actual output. \n"," - `Optimizer` is used to update the weights of the neural network to improve its performance."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"cb8f194ee79d76356be0002b0e18f947e1412d66","id":"HMqQTafXEaei"},"source":["class RobertaClass(torch.nn.Module):\n","    def __init__(self):\n","        super(RobertaClass, self).__init__()\n","        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 5)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ55mIPZIkp_","outputId":"82657080-cec9-4a26-d8f4-6a0c7075316c","colab":{"base_uri":"https://localhost:8080/","height":944,"referenced_widgets":["121a2cb0dc674ec7a1cecde46c12b8bf","796bd18f5b7748ffa1bcca749b0b1d26","861498f14a524367bba0853816c87b52","eb57666612e64c2ab1e68d3db631571e","174073ef46ef4a558fb00dbcb01a8c18","0c43d9fa9cef49a78b6e6b270acaec86","107a6093636645e8af4837fec96a7909","a5c5bc917b82423c8c925682e15313e4","b362a762ae504bfda906f651f59ffa0a","908ff024d1e24521b7b9d832cbeb38d9","d5ef88169455412997145c5d8cdd082d"]},"executionInfo":{"status":"ok","timestamp":1680988477099,"user_tz":300,"elapsed":9282,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["model = RobertaClass()\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121a2cb0dc674ec7a1cecde46c12b8bf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaClass(\n","  (l1): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"gsRa7gY3qE0n"},"source":["<a id='section05'></a>\n","### Fine Tuning the Model"]},{"cell_type":"code","metadata":{"id":"XYZ7YuJ5InOS"},"source":["# Creating the loss function and optimizer\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPhA2V3iIpzN"},"source":["def calcuate_accuracy(preds, targets):\n","    n_correct = (preds==targets).sum().item()\n","    return n_correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhqvtY2SIup7"},"source":["# Defining the training function on the 80% of the dataset for tuning the distilbert model\n","\n","def train(epoch):\n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    model.train()\n","    for _,data in tqdm(enumerate(training_loader, 0)):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.long)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calcuate_accuracy(big_idx, targets)\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","        \n","        if _%5000==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            accu_step = (n_correct*100)/nb_tr_examples \n","            print(f\"Training Loss per 5000 steps: {loss_step}\")\n","            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        # # When using GPU\n","        optimizer.step()\n","\n","    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Training Loss Epoch: {epoch_loss}\")\n","    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n","\n","    return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Afn7xaunJHnI","outputId":"55034f17-1593-436d-d840-305cc32223d1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680989305193,"user_tz":300,"elapsed":828102,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["EPOCHS = 3\n","for epoch in range(EPOCHS):\n","    train(epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","1it [00:02,  2.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 1.5592403411865234\n","Training Accuracy per 5000 steps: 37.5\n"]},{"output_type":"stream","name":"stderr","text":["797it [04:37,  2.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch 0: 82.22640916941435\n","Training Loss Epoch: 0.475050151446271\n","Training Accuracy Epoch: 82.22640916941435\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00,  6.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.09896615147590637\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["797it [04:35,  2.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch 1: 91.67844245564453\n","Training Loss Epoch: 0.23410784364461787\n","Training Accuracy Epoch: 91.67844245564453\n"]},{"output_type":"stream","name":"stderr","text":["1it [00:00,  6.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Training Loss per 5000 steps: 0.03870934620499611\n","Training Accuracy per 5000 steps: 100.0\n"]},{"output_type":"stream","name":"stderr","text":["797it [04:35,  2.90it/s]"]},{"output_type":"stream","name":"stdout","text":["The Total Accuracy for Epoch 2: 93.90799183545298\n","Training Loss Epoch: 0.1652856371266162\n","Training Accuracy Epoch: 93.90799183545298\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"vOcgTsovqE1A"},"source":["<a id='section06'></a>\n","### Validating the Model"]},{"cell_type":"code","metadata":{"id":"bFiNcy16JLwt"},"source":["def valid(model, testing_loader):\n","    model.eval()\n","    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n","    with torch.no_grad():\n","        for _, data in tqdm(enumerate(testing_loader, 0)):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype = torch.long)\n","            outputs = model(ids, mask, token_type_ids).squeeze()\n","            loss = loss_function(outputs, targets)\n","            tr_loss += loss.item()\n","            big_val, big_idx = torch.max(outputs.data, dim=1)\n","            n_correct += calcuate_accuracy(big_idx, targets)\n","\n","            nb_tr_steps += 1\n","            nb_tr_examples+=targets.size(0)\n","            \n","            if _%5000==0:\n","                loss_step = tr_loss/nb_tr_steps\n","                accu_step = (n_correct*100)/nb_tr_examples\n","                print(f\"Validation Loss per 100 steps: {loss_step}\")\n","                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accu = (n_correct*100)/nb_tr_examples\n","    print(f\"Validation Loss Epoch: {epoch_loss}\")\n","    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n","    \n","    return epoch_accu\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcUylInzKdV-","outputId":"946ff484-f7e7-4142-8c6b-64bbc88a680d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680989327573,"user_tz":300,"elapsed":22398,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["acc = valid(model, testing_loader)\n","print(\"Accuracy on test data = %0.2f%%\" % acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2it [00:00, 16.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Validation Loss per 100 steps: 0.9657677412033081\n","Validation Accuracy per 100 steps: 75.0\n"]},{"output_type":"stream","name":"stderr","text":["398it [00:23, 17.20it/s]"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Epoch: 0.2940660497494248\n","Validation Accuracy Epoch: 89.38442211055276\n","Accuracy on test data = 89.38%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"tZgO6C1BqE1a"},"source":["<a id='section07'></a>\n","### Saving the Trained Model Artifacts for inference"]},{"cell_type":"code","metadata":{"id":"8eKt004BKjyT","outputId":"a9a7a1c9-905c-4705-a346-7f1d33b6b589","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680989330574,"user_tz":300,"elapsed":3019,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}}},"source":["output_model_file = 'pytorch_roberta_sentiment.bin'\n","output_vocab_file = './'\n","\n","model_to_save = model\n","torch.save(model_to_save, output_model_file)\n","tokenizer.save_vocabulary(output_vocab_file)\n","\n","print('All files saved')\n","print('This tutorial is completed')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n","This tutorial is completed\n"]}]},{"cell_type":"markdown","source":["<a id='section08'></a>\n","### Make Predictions using the model"],"metadata":{"id":"YkDahyP4zQqQ"}},{"cell_type":"code","source":["# load actual unlabeled data\n","data_1 = pd.read_csv('FB_posts_unlabeled.txt', delimiter='\\t')\n","data_1['label'] = 0\n","data_1 = data_1.rename(columns={'message':'Phrase', 'label':'Sentiment'})\n","new_df_1 = data_1[['Phrase', 'Sentiment']]\n","\n","unlabled_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 0\n","                }\n","\n","unlabeled_set = SentimentData(new_df_1, tokenizer, MAX_LEN)\n","unlabeled_data_loader = DataLoader(unlabeled_set, **unlabled_params)"],"metadata":{"id":"9VJxaV7n2zC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n","\n","results = []\n","\n","with torch.no_grad():\n","    for _, data in tqdm(enumerate(unlabeled_data_loader, 0)):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","        targets = data['targets'].to(device, dtype = torch.long)\n","        outputs = model(ids, mask, token_type_ids).squeeze()\n","\n","        results.append(outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9cGTN66zF7p","executionInfo":{"status":"ok","timestamp":1680989357052,"user_tz":300,"elapsed":26483,"user":{"displayName":"Simin Liao","userId":"00207828707636711667"}},"outputId":"f93e9733-2f3f-4dcd-db9c-f6e4d8928ede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","510it [00:27, 18.33it/s]\n"]}]},{"cell_type":"code","source":["# Process the output to readable csv file\n","results_df = []\n","for i in results:\n","  for j in i:\n","    results_df.append(j)\n","\n","pred = []\n","for t in results_df:\n","  arr = t.cpu().numpy()\n","  idx = np.argmax(arr)\n","  pred.append(idx)\n","\n","data_1['pred'] = pred\n","\n","data_1['Appreciation_pred'] = np.where(data_1['pred']==1, 1, 0)\n","data_1['Complaint_pred'] = np.where(data_1['pred']==2, 1, 0)\n","data_1['Feedback_pred'] = np.where(data_1['pred']==3, 1, 0)"],"metadata":{"id":"_CPjdARl4lF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_1[['postId', 'Appreciation_pred', 'Complaint_pred', 'Feedback_pred']].to_csv('output.csv',index=False)"],"metadata":{"id":"b1g-cWEe4vhm"},"execution_count":null,"outputs":[]}]}